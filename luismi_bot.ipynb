{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luismi Bot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Ah, ah, ah, ah\\nAh, ah, ah, ah\\nAh, ah, ah, ah\n",
       "1    Quero, te abraçar apertado\\nTe quero, te apert...\n",
       "2    Quero, este fogo me queima\\nTe quero, cada hor...\n",
       "3    Somos dois, dois apaixonados\\nNeste amor, doce...\n",
       "4    Somos dois, dois apaixonados\\nEu, você, é mara...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leemos el archivo csv con las letras de Luis Miguel\n",
    "dataset = pd.read_csv('luis_miguel_lyrics.csv')\n",
    "\n",
    "# Guardamos solo la columna de lyrics\n",
    "dataset = dataset['lyrics']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Ah, ah, ah, ah\\nAh, ah, ah, ah\\nAh, ah, ah, ah\n",
       "1    Quero, te abraar apertado\\nTe quero, te aperta...\n",
       "2    Quero, este fogo me queima\\nTe quero, cada hor...\n",
       "3    Somos dois, dois apaixonados\\nNeste amor, doce...\n",
       "4    Somos dois, dois apaixonados\\nEu, voc, e marav...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Reemplazamos las vocales acentuadas por vocales sin acento y eliminamos los caracteres\n",
    "# que no se encuentren en el rango de caracteres [a-zA-Z0-9ñÑ,.:;?[]()!\"\\'‘’“”…¡¿\\n ]\n",
    "# incluyendo mayúsculas, minúsculas y espacios, en nueustro dataset de canciones\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = re.sub(r'[ÁÀ]', 'A', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[áà]', 'a', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[ÉÈËЕ]', 'E', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[éèëе]', 'e', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[ÍÌ]', 'I', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[íì]', 'i', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[ÓÒŌ]', 'O', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[óòō]', 'o', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[ÚÙÜ]', 'U', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[úùü]', 'u', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[ćč]', 'c', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[ĆČ]', 'c', str(dataset[i]))\n",
    "    dataset[i] = re.sub(r'[^a-zA-Z0-9ñÑ,.:;?[\\]()!\"\\'‘’“”…¡¿\\n ]', '', str(dataset[i]))\n",
    "\n",
    "dataset.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo convertimos a un archivo de texto\n",
    "dataset.to_csv('lyrics.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ah, ah, ah, ah\n",
      "Ah, ah, ah, ah\n",
      "Ah, ah, ah, ah\"\n",
      "\"Quero, te abraar apertado\n",
      "Te quero, te apertar no meu corpo\n",
      "Te quero, te falar deste amor\n",
      "E beijar voc pela vida inteira\"\n",
      "\"Quero, este fogo me queima\n",
      "Te quero, cada hora que passa\n",
      "Te espero, de repente o que importa\n",
      "E que somos dois, dois apaixonados\"\n",
      "\"Somos dois, dois apaixonados\n",
      "Neste amor, doce, limpo e claro\n",
      "E nos dois, vamos pouco a pouco\n",
      "Descobrindo todo o amor do mundo\"\n",
      "\"Somos dois, dois apaixonados\n",
      "Eu, voc, e maravilhoso\n",
      "Encontrar a felicidade\n",
      "Que nos dois achamos um no outro\"\n",
      "\"Ah, ah, ah, ah\n",
      "Ah, ah, ah, ah\n",
      "Ah, ah, ah, ah\"\n",
      "\"Quero, te abraar apertado\n",
      "Te quero, te apertar no meu corpo\n",
      "Te quero, te falar deste amor\n",
      "E beijar voc pela vida inteira\"\n",
      "\"Somos dois, dois apaixonados\n",
      "Neste amor, doce, limpo e claro\n",
      "E nos dois, vamos pouco a pouco\n",
      "Descobrindo todo o amor do mundo\"\n",
      "\"Somos dois, dois apaixonados\n",
      "Eu, voc, e maravilhoso\n",
      "Encontrar a felicidade\n",
      "Que nos dois achamos um no outro\"\n",
      "\"Ah, ah, ah, ah\n",
      "Ah, ah, ah, ah\n",
      "Ah, ah, ah, ah\"\n",
      "\"Te des\n"
     ]
    }
   ],
   "source": [
    "# Abriendo el archivo de texto\n",
    "with open('lyrics.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Imprimimos un fragmento del texto\n",
    "print(data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),.023:;?[]abcdefghijklmnopqrstuvwxyz¡¿ñ…'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimimos todos los caracteres que se encuentran en el texto\n",
    "\"\".join(sorted(set(data.lower())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Creamos un tokenizador\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 3, 1, 8, 7, 15, 4, 7, 13, 8, 15, 8, 4, 7, 3, 11]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['la incondicional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l a   i n c o n d i c i o n a l']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[11, 3, 1, 8, 7, 15, 4, 7, 13, 8, 15, 8, 4, 7, 3, 11]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el número de caracteres únicos y el total de caracteres\n",
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([16,  2, 21, 23,  0,  2, 21, 23,  0,  2, 21, 23,  0,  2, 21,  9,  2,\n",
      "       21, 23,  0,  2, 21, 23,  0,  2, 21, 23,  0,  2, 21,  9,  2, 21, 23,\n",
      "        0,  2, 21, 23,  0,  2, 21, 23,  0,  2, 21, 16,  9, 16, 17,  8,  1,\n",
      "        5,  3, 23,  0, 11,  1,  0,  2, 20,  5,  2,  2,  5,  0,  2, 15,  1,\n",
      "        5, 11,  2, 12,  3,  9, 11,  1,  0, 17,  8,  1,  5,  3, 23,  0, 11,\n",
      "        1,  0,  2, 15,  1,  5, 11,  2,  5,  0,  6,  3,  0, 13,  1])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creamos un arreglo con todos los caracteres del texto\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([data])) - 1\n",
    "\n",
    "# Impresión de los primeros 100 caracteres\n",
    "print([encoded[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Creamos un dataset con los caracteres del texto\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el tamaño de la ventana\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "\n",
    "# Creamos un dataset con ventanas de tamaño 101\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos el dataset a un arreglo de numpy\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una semilla para el generador de números aleatorios\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el tamaño del batch\n",
    "batch_size = 32\n",
    "\n",
    "# Mezclamos el dataset y lo dividimos en batches\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset en entrenamiento y prueba\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 09:12:17.130219: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 47) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el tamaño del dataset\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6364/6364 [==============================] - 171s 27ms/step - loss: 2.0141\n",
      "Epoch 2/10\n",
      "6364/6364 [==============================] - 170s 27ms/step - loss: 1.8603\n",
      "Epoch 3/10\n",
      "6364/6364 [==============================] - 170s 27ms/step - loss: 1.8027\n",
      "Epoch 4/10\n",
      "6364/6364 [==============================] - 172s 27ms/step - loss: 1.7734\n",
      "Epoch 5/10\n",
      "6364/6364 [==============================] - 171s 27ms/step - loss: 1.7553\n",
      "Epoch 6/10\n",
      "6364/6364 [==============================] - 172s 27ms/step - loss: 1.8342\n",
      "Epoch 7/10\n",
      "6364/6364 [==============================] - 170s 27ms/step - loss: 1.8374\n",
      "Epoch 8/10\n",
      "6364/6364 [==============================] - 169s 26ms/step - loss: 1.7892\n",
      "Epoch 9/10\n",
      "6364/6364 [==============================] - 169s 26ms/step - loss: 1.7680\n",
      "Epoch 10/10\n",
      "6364/6364 [==============================] - 166s 26ms/step - loss: 1.7558\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Definimos el modelo\n",
    "model = tf.keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                        dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                        dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "# Compilamos el modelo\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = model.fit(dataset, epochs=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocessar el texto\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predecimos el siguiente caracter\n",
    "X_new = preprocess([\"la incondicional\"])\n",
    "Y_pred = model.predict(X_new)\n",
    "clases = np.argmax(Y_pred, axis=-1)\n",
    "tokenizer.sequences_to_texts(clases + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener el siguiente caracter\n",
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    Y_pred = model.predict(X_new, verbose=0)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(Y_pred) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar texto\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amor\n",
      "de la vida en la palabra si te se siento y en el corazon\"\n",
      "\"si te se siento y entre tu pelo y yo se tu pensar\n",
      "en tu pelo y te se si te soñe\n",
      "como si te soñe\n",
      "como el sol amor\n",
      "el sol amor\n",
      "de la vida en la pasado con las contigo\n",
      "y se al amor de la vida en la palabra si te siento y el sol como el sol\n",
      "cuando calienta el sol\n",
      "cuando calienta el sol\n",
      "cuando calienta el sol\n",
      "cuando calienta el sol\n",
      "cuando calienta el sol\n",
      "cuando calienta el sol\n",
      "cuando calienta el sol\n",
      "cuando calienta el sol\n",
      "cuando calienta el \n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"amor\", n_chars=500, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardado del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "model.save('luismi_bot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardamos el tokenizador\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
